<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Lennart Schulze</title> <meta name="author" content="Lennart Schulze"> <meta name="description" content="Lennart Schulze is a Research Assistant in AI at Columbia University. "> <meta name="keywords" content="Lennart Schulze, artificial intelligence, machine learning, robotics, computer vision, research, PhD"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lenn-arts.github.io/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6C%65%6E%6E%61%72%74%20.%20%73%63%68%75%6C%7A%65%20[%41%54]%20%63%6F%6C%75%6D%62%69%61%20.%20%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/lenn-arts" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/lennart-schulze" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Lennart Schulze </h1> <p class="desc">Computer Science Ph.D. student at <a href="https://cs.columbia.edu" rel="external nofollow noopener" target="_blank">Columbia University</a>.</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lennart_schulze-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lennart_schulze-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lennart_schulze-1400.webp"></source> <img src="/assets/img/lennart_schulze.jpg?501cc6f5dbc02418cac5aff1892bcb64" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="lennart_schulze.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p style="font-size:12px !important">New York, NY<br> lennart.schulze [at] columbia.edu </p> </div> </div> <div class="clearfix"> <p>I am a machine learning Ph.D. student, broadly interested in deep learning, 3D computer vision, and robotics. In particular, I am interested in building methods that can learn generalizable, multimodal, robust, explainable, and controllable neural scene representations of the world that allow embodied agents to reason about their environment with common sense. Naturally, robot perception, interaction, and manipulation are applications of these methods.</p> <p>Currently, I am a Graduate Research Assistant in Columbia’s CS department co-advised by Prof. Matei Ciocarlie and Prof. Carl Vondrick. Before that, I worked on dynamic neural fields with Prof. Hod Lipson in Columbia’s Creative Machines Lab. I was also a Visiting Research Fellow at MIT CSAIL, where I worked on OOD robustness for language and vision models with Prof. Dylan Hadfield-Menell. Prior to that, I worked on quantum machine learning for high-energy physics at IBM Research advised by Dr. Panos Barkoutsos in collaboration with CERN. At IBM, I also held several positions in ML engineering, data science, and AI governance.</p> <p>I am a Fellow of the German Academic Scholarship Foundation, of the German Academic Exchange Service (DAAD), and an ERP Scholar of the German Government.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Mar 19, 2024</th> <td> New preprint on <a href="https://arxiv.org/pdf/2403.05030.pdf" title="Defending Against Unforeseen Failure Modes with Latent Adversarial Training" rel="external nofollow noopener" target="_blank">arxiv</a>. We show how latent adversarial training improves robustness in vision and large language models. </td> </tr> <tr> <th scope="row">Jan 29, 2024</th> <td> Our <a href="https://arxiv.org/pdf/2310.03624" title="High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning" rel="external nofollow noopener" target="_blank">paper</a> on Dynamic neural fields for robot modeling got accepted to ICRA 2024. </td> </tr> <tr> <th scope="row">Nov 25, 2023</th> <td> Our paper on quantifiable explainability for computer vision models got accepted at NeurIPS 2023 XAIA. </td> </tr> <tr> <th scope="row">Aug 21, 2023</th> <td> Our paper on dynamic neural fields got accepted and selected for oral presentation at ICCV 2023 NeRF4ADR. </td> </tr> <tr> <th scope="row">May 29, 2023</th> <td> I am joining MIT CSAIL as a Visiting Research Fellow in the Algorithmic Alignment Group to work on OOD robustness of vision and language models. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICRA; ICCV NeRF</abbr></div> <div id="schulze2023high" class="col-sm-8"> <div class="title">High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning</div> <div class="author"> <em>Lennart Schulze</em>, and <a href="https://www.hodlipson.com/" rel="external nofollow noopener" target="_blank">Hod Lipson</a> </div> <div class="periodical"> <em>In International Conference on Robotics and Automation (ICRA) 2024;<br> ICCV Workshop on Neural Fields for Autonomous Driving and Robotics</em>, 2023 </div> <div class="periodical"> <span style="font-weight: bold; color:#eb4034">(Oral Presentation)</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://neural-fields.xyz/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2310.03624.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>A robot self-model is a task-agnostic representation of the robot’s physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot’s kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot’s workspace dimension. We demonstrate the capabilities of this model on a motion planning task as an exemplary downstream application.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schulze2023high</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schulze, Lennart and Lipson, Hod}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA) 2024;&lt;br/&gt; ICCV Workshop on Neural Fields for Autonomous Driving and Robotics}</span><span class="p">,</span>
  <span class="na">other</span> <span class="p">=</span> <span class="s">{(Oral Presentation)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://xai-in-action.github.io/" rel="external nofollow noopener" target="_blank">NeurIPS XAIA</a></abbr></div> <div id="schulze2023obey" class="col-sm-8"> <div class="title">ObEy: Quantifiable Object-based Explainability without Ground-Truth Annotations</div> <div class="author"> <em>Lennart Schulze*</em>, William Ho*, and <a href="https://www.cs.columbia.edu/~zemel/" rel="external nofollow noopener" target="_blank">Richard Zemel</a> </div> <div class="periodical"> <em>In NeurIPS Workshop on Explainable AI in Action: Past, Present, and Future Applications</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://xai-in-action.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=N5RmOXuTDo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Neural networks are at the core of AI systems recently observing accelerated adoption in high-stakes environments. Consequently, understanding their black-box predictive behavior is paramount. Current explainable AI techniques, however, are limited to explaining a single prediction, rather than characterizing the inherent ability of the model to be explained, reducing their usefulness to manual inspection of samples. In this work, we offer a conceptual distinction between explanation methods and explainability. We use this motivation to propose Object-based Explainability (ObEy), a novel model explainability metric that collectively assesses model-produced saliency maps relative to objects in images, inspired by humans’ perception of scenes. To render ObEy independent of the prediction task, we use full-image instance segmentations obtained from a foundation model, making the metric applicable on existing models in any setting. We demonstrate ObEy’s immediate applicability to use cases in model inspection and comparison. As a result, we present new insights into the explainability of adversarially trained models from a quantitative perspective.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schulze2023obey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ObEy}: Quantifiable Object-based Explainability without Ground-Truth Annotations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schulze*, Lennart and Ho*, William and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{NeurIPS} Workshop on Explainable {AI} in Action: Past, Present, and Future Applications}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="casper2024defending" class="col-sm-8"> <div class="title">Defending Against Unforeseen Failure Modes with Latent Adversarial Training</div> <div class="author"> Stephen Casper*, <em>Lennart Schulze*</em>, Oam Patel, and Dylan Hadfield-Menell</div> <div class="periodical"> <em>arXiv preprint arXiv:2403.05030</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2403.05030.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red- teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">casper2024defending</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Defending Against Unforeseen Failure Modes with Latent Adversarial Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Casper*, Stephen and Schulze*, Lennart and Patel, Oam and Hadfield-Menell, Dylan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2403.05030}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6C%65%6E%6E%61%72%74%20.%20%73%63%68%75%6C%7A%65%20[%41%54]%20%63%6F%6C%75%6D%62%69%61%20.%20%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/lenn-arts" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/lennart-schulze" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> Please contact me via e-mail: lennart . schulze <at> columbia.edu </at> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Lennart Schulze. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: August 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>