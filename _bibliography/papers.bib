---
---

@string{iccv = {International Conference on Computer Vision}}
@string{neurips = {Conference on Neural Information Processing Systems}}

@inproceedings{schulze2023high,
  title={High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning},
  author={Schulze, Lennart and Lipson, Hod},
  year={2023},
booktitle={ICCV Workshop on Neural Fields for Autonomous Driving and Robotics},
  bibtex_show={true},
abbr={ICCV NeRF4ADR},
selected={true},
html={https://neural-fields.xyz/},
  pdf={https://arxiv.org/pdf/2310.03624.pdf},
other={Oral presentation.}
abstract={A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2\% of the robot's workspace dimension. We demonstrate the capabilities of this model on a motion planning task as an exemplary downstream application.}
}

@inproceedings{schulze2023obey,
  title={{ObEy}: Quantifiable Object-based Explainability without Ground-Truth Annotations},
  author={Schulze*, Lennart and Ho*, William and Zemel, Richard},
  booktitle={{NeurIPS} Workshop on Explainable {AI} in Action: Past, Present, and Future Applications},
  year={2023},
  bibtex_show={true},
abbr={NeurIPS XAIA},
selected={true},
html={https://xai-in-action.github.io/},
  pdf={https://openreview.net/pdf?id=N5RmOXuTDo},
abstract={Neural networks are at the core of AI systems recently observing accelerated adoption in high-stakes environments. Consequently, understanding their black-box predictive behavior is paramount. Current explainable AI techniques, however, are limited to explaining a single prediction, rather than characterizing the inherent ability of the model to be explained, reducing their usefulness to manual inspection of samples. In this work, we offer a conceptual distinction between explanation methods and explainability. We use this motivation to propose Object-based Explainability (ObEy), a novel model explainability metric that collectively assesses model-produced saliency maps relative to objects in images, inspired by humans’ perception of scenes. To render ObEy independent of the prediction task, we use full-image instance segmentations obtained from a foundation model, making the metric applicable on existing models in any setting. We demonstrate ObEy’s immediate applicability to use cases in model inspection and comparison. As a result, we present new insights into the explainability of adversarially trained models from a quantitative perspective.}
}
